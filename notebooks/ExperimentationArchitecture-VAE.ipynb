{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook with VAE model and no conditionning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#import external libraries\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths in git\n",
    "\n",
    "#root git folder \n",
    "#path_main_folder = '/home/marotant/dev/Autoencoder_Embedding_Expert_Caracteristion_'\n",
    "path_main_folder = '/home/jovyan'#specify the root folder of the git repo\n",
    "\n",
    "#add  to path root git folder \n",
    "sys.path.append(path_main_folder)\n",
    "#add  to path source code folder\n",
    "sys.path.append(path_main_folder+'/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import class and methods from src\n",
    "from keras import backend as K\n",
    "from CVAE.callbacks import NEpochLogger,callbackWeightLoss\n",
    "#from CVAE.cvae import compile_cvae, run_cvae\n",
    "from CVAE.cvae_model import CVAE, CVAE_emb, CAE\n",
    "from conso.load_shape_data import *  \n",
    "\n",
    "import Visualisation.buildProjector\n",
    "from Visualisation.buildProjector import *\n",
    "from FeaturesScore.scoring import *\n",
    "#from conso.load_shape_data import get_x_conso_autoencoder\n",
    "from conso.conso_helpers import plot_latent_space_projection, pyplot_latent_space_projection_temp, pyplot_latent_space_projection_error\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directories to store trained model and the related projector\n",
    "\n",
    "log_dir_projector=path_main_folder+\"/notebooks/logs/Expe1/VAE/projector\"\n",
    "log_dir_model=path_main_folder+\"/notebooks/logs/Expe1/VAE/model\"\n",
    "if not(os.path.isdir(log_dir_projector)):\n",
    "    os.makedirs(log_dir_projector)\n",
    "if not(os.path.isdir(log_dir_model)):\n",
    "    os.makedirs(log_dir_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents:\n",
    "- Load Data\n",
    "- Make Training Set\n",
    "- Define and Train Model\n",
    "- Build Projector\n",
    "- Compute Feature Scores in latent space\n",
    "- Study reconstruction Error\n",
    "- Study Holidays prediction\n",
    "- Detect atypical events\n",
    "- Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "path_data = os.path.join(path_main_folder, 'data')\n",
    "dataset_csv = os.path.join(path_data, \"dataset.csv\")\n",
    "x_conso = pd.read_csv(dataset_csv, sep=\",\",)\n",
    "x_conso.ds = pd.to_datetime(x_conso.ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#drop indices column\n",
    "x_conso=x_conso.drop(columns=x_conso.columns[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize data frame head\n",
    "x_conso.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make training set of daily electrical consumption profiles and conditions \n",
    "In this experiment there is no condition to pass. This is not something we can do with a PCA anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_set_plot = 'train'\n",
    "version = '-v1'\n",
    "nPoints=1830"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_xconso = {'train': x_conso}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize input variables\n",
    "type_scaler = 's'\n",
    "dict_xconso, _ = normalize_xconso(dict_xconso, type_scaler = 'standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset_autoencoder(dict_xconso=dict_xconso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A VAE is a CVAE with a null feature as a condition  \n",
    "\n",
    "#Pass a constant vector for conditions\n",
    "nPoints=dataset['train']['x'][1].shape[0]\n",
    "x = dataset['train']['x'][0]\n",
    "cond_pre =np.zeros((nPoints,), dtype=int)#\n",
    "\n",
    "dataset['train']['x'] = [x,cond_pre]\n",
    "dataset['train']['y1'] = dataset['train']['y'] #the CVAE model has two outputs which are identifcal to compute the two losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_info = pd.DataFrame(dataset[name_set_plot]['ds'])\n",
    "calendar_info['month'] = calendar_info.ds.dt.month\n",
    "calendar_info['weekday'] = calendar_info.ds.dt.weekday\n",
    "calendar_info['is_weekday'] = (calendar_info.weekday < 5).apply(lambda x:int(x))\n",
    "calendar_info = pd.merge(calendar_info, x_conso[['ds', 'is_holiday_day']], on='ds', how ='left')\n",
    "calendar_info.loc[calendar_info['is_holiday_day'].isna(),'is_holiday_day'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and train model VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#on sauvegarde le dataset\n",
    "path_out = log_dir_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for autoencoder\n",
    "e_dims=[48,35,24,12]#encoder dim\n",
    "d_dims=[48,35,24,12]#decoder dim. Dense Blocks in skip connections can make the dimensions bigger when layers are concatenated with the previous one\n",
    "to_emb_dim=[]\n",
    "cond_pre_dim = 0#dataset['train']['x'][1].shape[1]\n",
    "input_dim = dataset['train']['x'][0].shape[1]\n",
    "z_dim= 4\n",
    "lambda_val = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model = 'vae_conso-30min-journalier-nocond'\n",
    "#name_model = 'cvae_classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if needs to relaod model classes after modification wothout restarting the kernel\n",
    "\n",
    "import CVAE.cvae_model\n",
    "import CVAE.callbacks\n",
    "import importlib\n",
    "importlib.reload(CVAE.cvae_model)\n",
    "importlib.reload(CVAE.callbacks)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = K.variable(lambda_val, dtype='float32')\n",
    "model = CVAE.cvae_model.CVAE(input_dim=input_dim,\n",
    "             cond_dim=1, \n",
    "             e_dims=e_dims, \n",
    "             d_dims=d_dims, \n",
    "             z_dim=z_dim, \n",
    "             beta=Lambda,\n",
    "             name=name_model, \n",
    "             output=path_out,is_L2_Loss=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainning model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "\n",
    "#embeddingsMetadata = {'dec_dense_0': 'metadata.tsv'}\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(name_model +str(time())),write_graph=True)#,write_images=True,embeddings_freq=10, embeddings_layer_names=['dec_dense_0'],embeddings_metadata= embeddingsMetadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.main_train(dataset, training_epochs=200, batch_size=20, verbose=False,callbacks=[tensorboard])\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lambda_decreaseRate=0.0\n",
    "lambda_min=0.01\n",
    "\n",
    "out_batch = NEpochLogger(x_train_data=dataset['train']['x'], display=100,x_conso=x_conso,calendar_info=calendar_info)\n",
    "weightLoss=callbackWeightLoss(lambda_val,lambda_decreaseRate,lambda_min)\n",
    "#model.main_train(dataset, training_epochs=1500, batch_size=40, verbose=False,callbacks=[tensorboard,out_batch])#,weightLoss])\n",
    "model.main_train(dataset, training_epochs=400, batch_size=40, verbose=0,callbacks=[tensorboard,out_batch],validation_split=0.1)\n",
    "\n",
    "#visualizer = LatentSpaceVisualizer(model_folder_path=model_path, dataset_path=labellisation_data_folder + 'sequences_dataset/sequences_et_labels.npz')\n",
    " #   visualizer.visualize_embedding_after_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_decreaseRate=0.001\n",
    "weightLoss=callbackWeightLoss(lambda_val,lambda_decreaseRate,lambda_min)\n",
    "#model.main_train(dataset, training_epochs=1500, batch_size=40, verbose=False,callbacks=[tensorboard,out_batch])#,weightLoss])\n",
    "model.main_train(dataset, training_epochs=2000, batch_size=40, verbose=0,callbacks=[tensorboard,out_batch,weightLoss],validation_split=0.1)\n",
    "\n",
    "#visualizer = LatentSpaceVisualizer(model_folder_path=model_path, dataset_path=labellisation_data_folder + 'sequences_dataset/sequences_et_labels.npz')\n",
    " #   visualizer.visualize_embedding_after_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DimsImportance=[1394.4407  1343.6127    44.23878 1616.7899 ] Only 3 dimensions are significant here (each term is the sum of absolute values in each direction for the all the datapoints.\n",
    "There is no significant overfitting when comparing training error to validation error. This will be confimed later on specific examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_out,name_model,\"config.txt\"),'w') as file: \n",
    "    file.write(str(cond_pre_dim) + '\\n')\n",
    "    #file.write(str(emb_dims) + '\\n')\n",
    "    file.write(str(e_dims) + '\\n') \n",
    "    file.write(str(d_dims) + '\\n') \n",
    "    file.write(str(z_dim) + '\\n')\n",
    "    file.write(str(Lambda) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sauvegarde du dataset associé\n",
    "name_dataset = 'dataset.pickle'\n",
    "\n",
    "with open( os.path.join(path_out,name_model, name_dataset), \"wb\" ) as file:\n",
    "    pickle.dump( dataset, file )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_model(os.path.join(path_out, name_model, 'models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#cond  = dataset['train']['x'][1]\n",
    "x_input = dataset['train']['x'][0]\n",
    "\n",
    "input_encoder = [x_input, cond_pre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_encoded = model.encoder.predict(input_encoder)[0]\n",
    "x_hat = model.cvae.predict(x=dataset['train']['x'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the latent space with the construction of a tensorboard projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nPoints=1500 #if you want to visualize images of consumption profiles and its recontruction in tensorboard, there is a maximum size that can be handle for a sprite image. 1830 is  \n",
    "import os,cv2\n",
    "x_encoded_reduced=x_encoded[0:nPoints,]\n",
    "images=createLoadProfileImages(x,x_hat,nPoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sprites=images_to_sprite(images)\n",
    "cv2.imwrite(os.path.join(log_dir_projector, 'sprite_4_classes.png'), sprites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writeMetaData(log_dir_projector,x_conso,calendar_info,nPoints,has_Odd=False)\n",
    "buildProjector(x_encoded_reduced,images=images, log_dir=log_dir_projector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Features in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noCond_VAE=predictFeaturesInLatentSPace(x_conso,calendar_info,x_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reconstruction error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error=np.sum(np.abs((x - x_hat)),axis=1)/48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a histogram over residuals\n",
    "import seaborn as sn\n",
    "sn.distplot(error, kde=False, fit=stats.norm, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the day with errors above a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorThreshold=0.08\n",
    "idxMaxError=[i for i in range(0,nPoints) if error[i]>=ErrorThreshold]\n",
    "calender_error=calendar_info.loc[idxMaxError]\n",
    "calender_error['error']=error[idxMaxError]\n",
    "\n",
    "calender_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the first n days with highest errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nDays=30\n",
    "\n",
    "decreasingOrderIdx=np.argsort(-error)\n",
    "calendar_Error_Highest=calendar_info.loc[decreasingOrderIdx[0:nDays]]\n",
    "calendar_Error_Highest['error']=error[decreasingOrderIdx[0:nDays]]\n",
    "calendar_Error_Highest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the reconstruction error over a specific day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice=1185 #1185 is the changing hour day end of march\n",
    "fig = plt.figure(dpi=100,figsize=(3,3))\n",
    "#set(gca,'Color','k')\n",
    "plt.plot(x[indice,:])\n",
    "plt.plot(x_hat[indice,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the reconstruction error over the days with highest error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nPlots=10#len(idxMaxError)\n",
    "nCols=5\n",
    "nRows=int(nPlots/nCols)+1\n",
    "fig = plt.figure(dpi=100,figsize=(10,10))\n",
    "for i in range(1, nPlots):\n",
    "    plt.subplot(nRows, nCols, i)\n",
    "    fig.subplots_adjust(hspace=.5)\n",
    "    indice=decreasingOrderIdx[i-1]\n",
    "    plt.plot(x[indice,:])\n",
    "    plt.plot(x_hat[indice,:])\n",
    "    plt.title( calendar_Error_Highest.ds.dt.date.iloc[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2013-03-31 is the day with a missing hour because of changing day time and the consumption value is set to 0. It is hence normal that it is not well predicted and a good indicator that the model does not tend to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study of holiday predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparation des features d'interet\n",
    "yHd=calendar_info['is_holiday_day'].astype(int)\n",
    "indicesHd=np.array([i for i in range(0, nPoints) if yHd[i] == 1])\n",
    "yHd_only=yHd[yHd==1]\n",
    "x_encoded_Hd=x_encoded[indicesHd,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "results_hd=scoreKnnResults(x_encoded,yHd,type='classifier',k=5,cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## holidays well predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_hd_only=[results_hd['predP'][i] for i in indicesHd ]\n",
    "indices_Hd_predict=[i for i in indicesHd if  results_hd['predP'][i]>=0.5]\n",
    "indices_Hd_not_predicted=[i for i in indicesHd if  results_hd['predP'][i]<0.5]\n",
    "calendar_info.loc[indices_Hd_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "yWeekday=calendar_info['is_weekday']\n",
    "results_wk=scoreKnnResults(x_encoded,yWeekday,type='classifier',k=10,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays_predicted_as_weekend=[i for i in range(0,1830) if  results_wk['predP'][i]<=0.5 and yWeekday[i]==1]\n",
    "calendar_info.loc[weekdays_predicted_as_weekend]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weekdays_predicted_as_weekend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find out that holidays actually look alike weekends even if they are happening during weekdays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holidays & nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(10)\n",
    "neigh.fit(x_encoded)\n",
    "\n",
    "[distance_knn,kneighbors]=neigh.kneighbors(x_encoded, 2, return_distance=True)\n",
    "nearest=distance_knn[:,1]\n",
    "fig = plt.figure(dpi=100,figsize=(3,3))\n",
    "plt.hist(nearest,bins=100)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "stats.describe(nearest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=100,figsize=(3,3))\n",
    "plt.hist(nearest[indicesHd],bins=100)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.describe(nearest[indicesHd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_info.loc[np.where(nearest>=0.7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2013-01-18 and 2017-01-21 were big snowy events in France and first of january are alwaus atypical days. 2014-03-28 is a day time changing hour day with a bad data for the additional fictitious hour. All of those events happened durng winter, when the electrical consumption is most sensitive to temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicesNear=[i for i in range(0,len(nearest)) if nearest[i]>=0.7]\n",
    "nearest[np.where(nearest>=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nPlots=len(indicesNear)#len(idxMaxError)\n",
    "nCols=5\n",
    "nRows=int(nPlots/nCols)+1\n",
    "fig = plt.figure(dpi=100,figsize=(10,10))\n",
    "for i in range(1, nPlots+1):\n",
    "    plt.subplot(nRows, nCols, i)\n",
    "    fig.subplots_adjust(hspace=.5)\n",
    "    indice=indicesNear[i-1]\n",
    "    plt.plot(x[indice,:])\n",
    "    plt.plot(x_hat[indice,:])\n",
    "    plt.title( calendar_info.ds.dt.date.iloc[indice])\n",
    "fig.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "- 3 dimensions covers most of the information for the variety of daily load curves \n",
    "- We recovered with this simple linear model the two main features that caracterizes electrical consumption: weekday and temperature\n",
    "- Holidays are not yet well predicted and represented, although we know they are an important atypical factor.\n",
    "- We however detect that holidays all look alike weekend days\n",
    "- We discover some first interpretable events.\n",
    "- Results are quite similar than with the PCA model. They are some differences in the events first detected. The projection in the latent space is also more gaussian, which ease the interactive navigation in it, compared to the PCA projection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
