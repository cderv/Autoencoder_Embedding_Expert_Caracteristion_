{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "  <hr> <center> <font size=\"+3.5\"> <b> Intrepreting atypical conditions in systems with conditional Autoencoders </b> </font> </center> <hr>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" role=\"alert\">\n",
    "    <center><b> <u>Auteur :</u>  Clement GOUBET  </b></center>\n",
    "</div>\n",
    "<div class=\"alert alert-block\" role=\"alert\">\n",
    "    <center> <font size=\"+1.5\"> <b>  23 juillet 2019  </b>  </font> </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <font size=\"+2\"> <b> 0. Preparation of the environment</b> </font> <hr>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading libraries and shapping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import external libraries\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "from scipy import stats\n",
    "import cv2 #from open-cv, to convert array to images\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths in git\n",
    "\n",
    "#root git folder \n",
    "path_main_folder = '/home/goubetcle/Documents/CVAE/marota_cvae'\n",
    "#path_main_folder = '/home/jovyan'#specify the root folder of the git repo\n",
    "\n",
    "#add  to path root git folder \n",
    "sys.path.append(path_main_folder)\n",
    "#add  to path source code folder\n",
    "sys.path.append(path_main_folder+'/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import class and methods from src\n",
    "from keras import backend as K\n",
    "from CVAE.callbacks import NEpochLogger,callbackWeightLoss\n",
    "#from CVAE.cvae import compile_cvae, run_cvae\n",
    "from conso.load_shape_data import *\n",
    "\n",
    "import Visualisation.buildProjector\n",
    "from Visualisation.buildProjector import *\n",
    "from FeaturesScore.scoring import *\n",
    "from FeaturesScore.exploration import *\n",
    "\n",
    "#from conso.load_shape_data import get_x_conso_autoencoder\n",
    "from conso.conso_helpers import plot_latent_space_projection, pyplot_latent_space_projection_temp, pyplot_latent_space_projection_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CVAE.cvae_model\n",
    "import CVAE.cvae_model_mixture\n",
    "import CVAE.cvae_model_cluster\n",
    "import importlib\n",
    "importlib.reload(CVAE.cvae_model)\n",
    "importlib.reload(CVAE.cvae_model_mixture)\n",
    "importlib.reload(CVAE.cvae_model_cluster)\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données de consommation et de température sont des prises de mesure par pas de temps 30 minutes pendant 5 années de décembre 2012 à décembre 2017. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>consumption_France</th>\n",
       "      <th>temperature_France</th>\n",
       "      <th>is_holiday_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87825</th>\n",
       "      <td>2017-12-31 21:30:00</td>\n",
       "      <td>56505.0</td>\n",
       "      <td>9.306475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87826</th>\n",
       "      <td>2017-12-31 22:00:00</td>\n",
       "      <td>56000.0</td>\n",
       "      <td>9.097250</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87827</th>\n",
       "      <td>2017-12-31 22:30:00</td>\n",
       "      <td>56995.0</td>\n",
       "      <td>9.048775</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87828</th>\n",
       "      <td>2017-12-31 23:00:00</td>\n",
       "      <td>60395.0</td>\n",
       "      <td>9.000300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87829</th>\n",
       "      <td>2017-12-31 23:30:00</td>\n",
       "      <td>60230.0</td>\n",
       "      <td>9.000300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ds  consumption_France  temperature_France  \\\n",
       "87825 2017-12-31 21:30:00             56505.0            9.306475   \n",
       "87826 2017-12-31 22:00:00             56000.0            9.097250   \n",
       "87827 2017-12-31 22:30:00             56995.0            9.048775   \n",
       "87828 2017-12-31 23:00:00             60395.0            9.000300   \n",
       "87829 2017-12-31 23:30:00             60230.0            9.000300   \n",
       "\n",
       "       is_holiday_day  \n",
       "87825             0.0  \n",
       "87826             0.0  \n",
       "87827             0.0  \n",
       "87828             0.0  \n",
       "87829             0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataframe\n",
    "path_data = os.path.join(path_main_folder, 'data')\n",
    "dataset_csv = os.path.join(path_data, \"dataset.csv\")\n",
    "x_conso = pd.read_csv(dataset_csv, sep=\",\",)\n",
    "x_conso.ds = pd.to_datetime(x_conso.ds)\n",
    "\n",
    "#drop indices column\n",
    "x_conso=x_conso.drop(columns=x_conso.columns[0],axis=1)\n",
    "\n",
    "#Visualize data frame head\n",
    "x_conso.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'intéresse ici à caractériser les profils journaliers. L'ensemble de données est donc transformé pour que notre jeu d'entrée soit un exemple de dimension ici 48 pour la consommation d'électricité. Avant cela, il est ici normalisé sur l'ensemble des mesures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['consumption_France']\n"
     ]
    }
   ],
   "source": [
    "name_set_plot = 'train'\n",
    "version = '-v1'\n",
    "nPoints=1830\n",
    "\n",
    "dict_xconso = {'train': x_conso}\n",
    "\n",
    "# Normalize input variables\n",
    "type_scaler = 's'\n",
    "dict_xconso, _ = normalize_xconso(dict_xconso, type_scaler = 'standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset_autoencoder(dict_xconso=dict_xconso)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dictionnaire dataset contient toutes les informations nécessaires à l'entrainement des modèles :\n",
    "- dataset['train']['x'] contient la liste des entrées de l'encodeur:\n",
    "-- [0] les profils de consommations\n",
    "-- [1] les conditions passées en entrée de l'encodeur et du décodeur pour rendre la représentation latente plus indépendante de ces variables\n",
    "- dataset['train']['y'] contient une copie des profils des consommation (pointeur pour le calcul des pertes de reconstruction dans l'apprentissage des modèles)\n",
    "- dataset['train']['ds'] contient les dates des exemples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour interpréter notre représentation, on peut chercher à visualiser des paramètres connus associés à nos profils journaliers. Ici il s'agit notamment de données calendaires (mois, weekend, jour férié), auxquels on peut rajouter la température moyenne observée sur la journée (par exemple), ou encore la pente moyenne du profil.\n",
    "\n",
    "Ces informations sont à passer dans calendar_info pour être visualisés dans une projection Tensorboard, et à passer tye et en valeur respectivement dans le dictionnaire factorDesc et factorMatrix pour servir dans l'évaluation des latents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_info = pd.DataFrame(dataset[name_set_plot]['ds'])\n",
    "calendar_info['month'] = calendar_info.ds.dt.month\n",
    "calendar_info['weekday'] = calendar_info.ds.dt.weekday\n",
    "calendar_info['is_weekday'] = (calendar_info.weekday < 5).apply(lambda x:int(x))\n",
    "calendar_info = pd.merge(calendar_info, x_conso[['ds', 'is_holiday_day']], on='ds', how ='left')\n",
    "calendar_info.loc[calendar_info['is_holiday_day'].isna(),'is_holiday_day'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Study conso variations as importance factors\n",
    "slope = np.abs(np.diff(dataset['train']['x'][0]))\n",
    "#variationMax = np.max(slope, axis=1)\n",
    "variationMean = np.mean(slope, axis = 1)\n",
    "#calendar_info['load_variation_max'] = variationMax\n",
    "calendar_info['load_variation_mean'] = variationMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1830.000000\n",
       "mean        0.008743\n",
       "std         0.093121\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.000000\n",
       "max         1.000000\n",
       "Name: potential_bridge_holiday, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#explicit the potential bridge days taken as extended holidays\n",
    "day_hol = calendar_info[['weekday', 'is_holiday_day']].copy().values\n",
    "bridge_index=[]\n",
    "for i in range(day_hol.shape[0]):\n",
    "    if day_hol[i,1]==1:\n",
    "        if day_hol[i,0]==1:\n",
    "            bridge_index.append(i-1)\n",
    "        elif day_hol[i,0]==3:\n",
    "            bridge_index.append(i+1)\n",
    "\n",
    "bridges = np.zeros(day_hol.shape[0])\n",
    "bridges[np.asarray(bridge_index)] = 1\n",
    "\n",
    "calendar_info['potential_bridge_holiday'] = bridges\n",
    "#calendar_info['potential_bridge_holiday'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = calendar_info.iloc[:,1:5].copy()\n",
    "columns_x = x_conso.columns\n",
    "conso_idx = np.argmax(['consumption' in c for c in x_conso.columns])\n",
    "temp_idx = np.argmax(['temperature' in c for c in x_conso.columns])\n",
    "dates = np.unique(x_conso['ds'].dt.date)\n",
    "temperatureMean=np.asarray([np.mean(x_conso[columns_x[temp_idx]].iloc[np.where(x_conso['ds'].dt.date==dates[k])]) for k in range(dates.shape[0])])\n",
    "\n",
    "factorMatrix = np.c_[factors.values,temperatureMean,variationMean]\n",
    "\n",
    "factorDesc={\n",
    "    'month':'category',\n",
    "    'weekday':'category',\n",
    "    'is_weekday':'category',\n",
    "    'is_holiday_day':'category',\n",
    "    'temperature':'regressor',\n",
    "    'load_delta_mean' : 'regressor'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools to evaluate the representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisation des metrics\n",
    "def display_metrics(model_eval, z_dim, factorDesc):\n",
    "    if 'reconstruction_error' in model_eval.keys():\n",
    "        for k,v in model_eval['reconstruction_error'].item():\n",
    "            print(k, ' : ', v)\n",
    "    print('Mutual Information Gap : %.2f'%model_eval['mig'])\n",
    "\n",
    "    fig = plt.figure(dpi=100,figsize=(10,8))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    plt.bar(factorDesc.keys(),model_eval['informativeness'])\n",
    "    plt.xlabel('factors')\n",
    "    plt.xticks(rotation=75)\n",
    "    plt.ylim(top=1)\n",
    "    for index,data in enumerate(model_eval['informativeness']):\n",
    "        plt.text(x=index - 0.5, y =data+0.01 , s=\"%.2f\"%data , fontdict=dict(fontsize=10))\n",
    "    plt.title('Informativeness score : %.2f'%np.mean(model_eval['informativeness']))\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.bar(np.arange(z_dim)+1,model_eval['disentanglement'])\n",
    "    plt.xlabel('latent variables')\n",
    "    plt.title('Disentanglement score : %.2f'%model_eval['mean_disentanglement']);\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.bar(factorDesc.keys(),model_eval['compactness'])\n",
    "    plt.xlabel('factors')\n",
    "    plt.xticks(rotation=75)\n",
    "    plt.title('Compactness')\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisation d'une représentation 2D dans le notebook\n",
    "from sklearn.decomposition import PCA\n",
    "def prez_2D(x_encoded):\n",
    "    proj2D = PCA(2)\n",
    "    proj = proj2D.fit_transform(x_encoded)\n",
    "    plt.figure(figsize=(36,18))\n",
    "    \n",
    "    #visualisation des mois par un disque de fond coloré\n",
    "    for i in np.unique(factorMatrix[:,1]):\n",
    "        i = int(i)\n",
    "        index = factorMatrix[:,1]==i\n",
    "        plt.scatter(x=proj[index,0], y=proj[index,1], c= factorMatrix[index,0], marker = 'o', s=500, alpha=0.5, cmap = 'Paired')\n",
    "\n",
    "    plt.colorbar().set_label('month');\n",
    "    \n",
    "    #visualisation de la température par la coloration graduelle du jour de la semaine\n",
    "    for i in np.unique(factorMatrix[:,1]):\n",
    "        i = int(i)\n",
    "        index = factorMatrix[:,1]==i\n",
    "        plt.scatter(x=proj[index,0], y=proj[index,1], c= temperatureMean[index], marker = '$'+str(i)+'$', s=200)\n",
    "\n",
    "    plt.colorbar().set_label('temperature');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour créer une représentation sous tensorboard, la fonction suivante regroupe toutes les étapes. Le booléen includeConsuptionProfileImages active ou non le marqueur des points à l'image des profils de consommation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "\n",
    "def tensorboardProjection(name_model,x, x_hat, x_encoded, x_conso, calendar_info, log_dir_projector, includeConsumptionProfileImages=True):\n",
    "    #can take a bit longer to create and load in tensorboard projector, but it looks better in the projector\n",
    "    if includeConsumptionProfileImages:\n",
    "        nPoints=1500 #if you want to visualize images of consumption profiles and its recontruction in tensorboard, there is a maximum size that can be handle for a sprite image. 1830 is  \n",
    "        x_encoded_reduced=x_encoded[0:nPoints,]\n",
    "        images=createLoadProfileImages(x,x_hat,nPoints)\n",
    "    else:\n",
    "        nPoints=1830\n",
    "        \n",
    "    if includeConsumptionProfileImages:\n",
    "        sprites=images_to_sprite(images)\n",
    "        cv2.imwrite(os.path.join(log_dir_projector, 'sprite_4_classes.png'), sprites)\n",
    "    \n",
    "    writeMetaData(log_dir_projector,x_conso,calendar_info,nPoints,has_Odd=False)\n",
    "    if includeConsumptionProfileImages:\n",
    "        buildProjector(x_encoded_reduced,images=images, log_dir=log_dir_projector)\n",
    "    else:\n",
    "        buildProjector(x_encoded,images=None, log_dir=log_dir_projector)\n",
    "        \n",
    "    print(log_dir_projector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <font size=\"+2\"> <b> 1. First classic model </b> </font> <hr>\n",
    "</div>\n",
    "\n",
    "## Build and train model VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres de l'autoencodeur\n",
    "z_dim = 4 # le nombre de dimensions voulues dans notre représentation latente\n",
    "e_dims=[48,35,24,12]# les couches cachées du bloc encodeur; premier nombre = inputs_dim\n",
    "d_dims=[48,35,24,12]# les couches cachées du bloc decodeur; premier nombre = outputs_dim\n",
    "lr=3e-4 # le learning rate de l'optimiseur\n",
    "input_dim = dataset['train']['x'][0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres du modèle variationnel choisi\n",
    "\n",
    "## pour l'erreur de reconstruction (entre L1 et L2)\n",
    "is_L2_Loss=False\n",
    "\n",
    "## pour le choix du prior univarié exponentiel entre 'Gaussian' et 'Laplace':\n",
    "prior = 'Laplace'\n",
    "\n",
    "## pour la régularisation de la fonction objectif \n",
    "Beta = K.variable(0.5, dtype='float32') # VAE classique --> beta = 1\n",
    "\n",
    "InfoVAE = True #pour activer la régularisation supplémentaire InfoVAE\n",
    "Gamma = K.variable(8, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres de l'entrainement du modèle\n",
    "epochs = 800\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du modèle\n",
    "name_model = 'model_vae'\n",
    "log_dir_projector=path_main_folder+\"/notebooks/logs/ModelComparison/VAE/projector/\"+name_model #pour créer le dossier du projeteur tensorboard\n",
    "log_dir_model=path_main_folder+\"/notebooks/logs/ModelComparison/VAE/model/\"+name_model #pour créer le dossier de sauvegarde du modèle\n",
    "if not(os.path.isdir(log_dir_projector)):\n",
    "    os.makedirs(log_dir_projector)\n",
    "if not(os.path.isdir(log_dir_model)):\n",
    "    os.makedirs(log_dir_model)\n",
    "path_out = log_dir_model\n",
    "\n",
    "model_vae = CVAE.cvae_model.CVAE(e_dims=e_dims, d_dims=d_dims, z_dim=z_dim, lr=lr,\n",
    "                                input_dim=input_dim, cond_dim=1, InfoVAE=InfoVAE,\n",
    "                                beta=Beta, gamma = Gamma,  is_L2_Loss=is_L2_Loss,\n",
    "                                name=name_model, output=path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apprentissage du modèle\n",
    " #use verbose=1 to see logs of training at every epoch\n",
    "\n",
    "model_vae.main_train(dataset, training_epochs=epochs, batch_size=batch_size, verbose=0,callbacks=[tensorboard],validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model and first interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération de la représentation latente via les distributions a posteriori des profils\n",
    "x_encoded, variance = model_vae.encoder.predict(dataset['train']['x'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval, importanceMatrix =evaluateLatentCode(x_encoded, factorMatrix, factorDesc)\n",
    "display_metrics(model_eval, z_dim, factorDesc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
